{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050fda4c-161e-4729-aabe-2bcadce80681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 410_data/matrix_14stations_365.csv shape = (14, 365)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Coordinate file (contain a column with station IDs)\n",
    "COORD_FILE = \"410_data/14_Station_Coordinates__Updated_.csv\"\n",
    "# Folder that contains all NCEI normals CSV files\n",
    "DATA_DIR   = \"410_data\"\n",
    "# Output file: stations × 365 matrix\n",
    "OUT_FILE   = \"410_data/matrix_14stations_365.csv\"\n",
    "\n",
    "\n",
    "coords = pd.read_csv(COORD_FILE)\n",
    "\n",
    "# Here we assume the coordinate table has a column named \"file\"\n",
    "# whose values look like USC00111577, USC00300183, ...\n",
    "station_ids = coords[\"file\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "def read_station_series(path):\n",
    "    \"\"\"\n",
    "    From a single NCEI Daily Climate Normals file, extract a 365-length\n",
    "    Tmean (DLY-TAVG-NORMAL) series indexed by day-of-year (1..365).\n",
    "\n",
    "    The function:\n",
    "      * finds a Tmean/TAVG normals column (case-insensitive),\n",
    "      * builds a day-of-year index (1..366),\n",
    "      * interpolates missing values,\n",
    "      * removes DOY 366 (leap day) and returns only 1..365.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "    #Find the Tmean column\n",
    "    t_candidates = []\n",
    "    for c in df.columns:\n",
    "        cname = str(c).lower()\n",
    "        if (\n",
    "            \"dly-tavg-normal\" in cname\n",
    "            or (\"tavg\" in cname and \"normal\" in cname)\n",
    "            or (\"tmean\" in cname and \"normal\" in cname)\n",
    "        ):\n",
    "            t_candidates.append(c)\n",
    "\n",
    "    if not t_candidates:\n",
    "        raise ValueError(f\"No Tmean-like column found in {os.path.basename(path)}\")\n",
    "\n",
    "    tcol = t_candidates[0]\n",
    "\n",
    "    # Build day-of-year: 1 - 366\n",
    "    cols_lower = {c: str(c).lower() for c in df.columns}\n",
    "    doy_col = None\n",
    "\n",
    "    #If the file already has a DOY/dayofyear/julian/yearday column, use it\n",
    "    for c in df.columns:\n",
    "        cl = cols_lower[c]\n",
    "        if cl in [\"doy\", \"dayofyear\", \"julian\", \"yearday\"]:\n",
    "            doy_col = c\n",
    "            break\n",
    "\n",
    "    if doy_col is not None:\n",
    "        sub = df[[doy_col, tcol]].copy()\n",
    "        sub[doy_col] = pd.to_numeric(sub[doy_col], errors=\"coerce\")\n",
    "        sub[tcol]    = pd.to_numeric(sub[tcol],    errors=\"coerce\")\n",
    "        sub = sub.dropna(subset=[doy_col, tcol])\n",
    "        sub = sub[(sub[doy_col] >= 1) & (sub[doy_col] <= 366)]\n",
    "        # If there are duplicate days, average them\n",
    "        sub = sub.groupby(doy_col)[tcol].mean()\n",
    "        series_full = pd.Series(index=range(1, 367), dtype=float)\n",
    "        series_full.loc[sub.index.astype(int)] = sub.values\n",
    "\n",
    "    # Otherwise, if there are month+day columns, compute DOY using year 2000\n",
    "    elif {\"month\", \"day\"}.issubset(set(df.columns)):\n",
    "        tmp = df[[\"month\", \"day\", tcol]].copy()\n",
    "        tmp[\"month\"] = pd.to_numeric(tmp[\"month\"], errors=\"coerce\")\n",
    "        tmp[\"day\"]   = pd.to_numeric(tmp[\"day\"],   errors=\"coerce\")\n",
    "        tmp[tcol]    = pd.to_numeric(tmp[tcol],    errors=\"coerce\")\n",
    "        tmp = tmp.dropna(subset=[\"month\", \"day\", tcol])\n",
    "\n",
    "        # Year 2000 is a leap year, so it can represent Feb 29 (DOY = 60)\n",
    "        tmp[\"doy\"] = pd.to_datetime(\n",
    "            {\n",
    "                \"year\": 2000,\n",
    "                \"month\": tmp[\"month\"].astype(int),\n",
    "                \"day\":   tmp[\"day\"].astype(int),\n",
    "            },\n",
    "            errors=\"coerce\",\n",
    "        ).dt.dayofyear\n",
    "\n",
    "        tmp = tmp.dropna(subset=[\"doy\"])\n",
    "        tmp[\"doy\"] = tmp[\"doy\"].astype(int)\n",
    "        tmp = tmp[(tmp[\"doy\"] >= 1) & (tmp[\"doy\"] <= 366)]\n",
    "\n",
    "        sub = tmp.groupby(\"doy\")[tcol].mean()\n",
    "        series_full = pd.Series(index=range(1, 367), dtype=float)\n",
    "        series_full.loc[sub.index] = sub.values\n",
    "\n",
    "    #Fallback: assume the file is already one row per day in order\n",
    "    else:\n",
    "        s = pd.to_numeric(df[tcol], errors=\"coerce\")\n",
    "        series_full = pd.Series(index=range(1, 367), dtype=float)\n",
    "        n = min(len(s), 366)\n",
    "        series_full.iloc[:n] = s.iloc[:n].values\n",
    "\n",
    "    #Interpolate and drop DOY 366 to get 365 days \n",
    "    series_full = series_full.interpolate(limit_direction=\"both\")\n",
    "\n",
    "    # Keep only DOY 1..365\n",
    "    series365 = series_full.iloc[:365]\n",
    "\n",
    "    # Safety check: if there are still NaNs, interpolate again\n",
    "    if series365.isna().any():\n",
    "        series365 = series365.interpolate(limit_direction=\"both\")\n",
    "\n",
    "    return series365\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Main loop: build stations × 365 matrix\n",
    "# ==============================\n",
    "\n",
    "rows = []\n",
    "good_ids = []\n",
    "\n",
    "for sid in station_ids:\n",
    "    f = os.path.join(DATA_DIR, f\"{sid}.csv\")\n",
    "    if not os.path.exists(f):\n",
    "        print(\"Missing file for station:\", sid)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        series365 = read_station_series(f)\n",
    "        if len(series365) != 365:\n",
    "            raise ValueError(f\"Got length {len(series365)} for {sid}\")\n",
    "        rows.append(series365.values)\n",
    "        good_ids.append(sid)\n",
    "    except Exception as e:\n",
    "        print(\"Skip\", sid, \"->\", e)\n",
    "\n",
    "if rows:\n",
    "    # Stack into a matrix: stations × 365\n",
    "    X = np.vstack(rows)\n",
    "    days = list(range(1, 366))\n",
    "\n",
    "    matrix_df = pd.DataFrame(X, index=good_ids, columns=days)\n",
    "\n",
    "    # Final QC: if any NaNs remain, interpolate along each row\n",
    "    if matrix_df.isna().any().any():\n",
    "        matrix_df = matrix_df.apply(\n",
    "            lambda r: pd.Series(r).interpolate(limit_direction=\"both\"), axis=1\n",
    "        )\n",
    "        matrix_df.index = good_ids\n",
    "        matrix_df.columns = days\n",
    "\n",
    "    matrix_df.to_csv(OUT_FILE)\n",
    "    print(\"Saved:\", OUT_FILE, \"shape =\", matrix_df.shape)\n",
    "else:\n",
    "    print(\"No station series were built; please check DATA_DIR and column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1aa28-1372-4d52-900f-97c02f9f86fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
